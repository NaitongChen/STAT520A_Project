% !TEX root = ../main.tex

% Background section

\section{Experiment}
As mentioned in the previous sections, both the length of the entire sequence and the number of change points to be detected have an impact on the number of possible configurations of change point locations and thus the computational cost of evaluating the conditional distribution of the change point locations. Therefore, to evaluate the performance of both MCMC-based Bayesian change point detection methods, inference on five sequences of varying lengths and number of true change points are conducted using both the Gibbs and MWG approaches. The number of possible change point configurations range from $49$ to $32509$. The detailed specification and visualization of the five sequences are shown in \cref{appendix:sequences}.\\\\
In this set of experiments, the $m$ value in \cref{eqn:prior} is set to be the average of the true segment means of each sequence specified in \cref{fig:sequences_viz}. Then inference on the change point locations of each sequence is run five times using both methods to ensure the results that we obtain are not by chance. For each of these runs, the same initial values are used for both the Gibbs sampler and the MWG sampler, although these initial values are not stored in the final output. Specifically, the initial change point locations are uniformly sampled and the initial segment means are the empirical means of the segments based on the initial change points. The Markov chains are run for one minute on the first and smallest time series, and for three minutes on all of the other four time series. Note that the relative time is measured using \texttt{time.perf\_counter} provided in Python with the time spent on storing each output sample excluded. It is worth noting that the \texttt{numba} package is used for both methods to speed up computations by translating and pre-compiling some of the \texttt{numpy} functions. The experiments are run with an Intel i5 7200U processor and 8GB of memory. Code is available at \url{https://github.com/NaitongChen/STAT520A_Project}.
\subsection{Discussion}
Firstly, by checking the plots shown in \cref{appendix:sample}, we confirm that the number of samples produced by MWG is indeed orders of magnitude larger than that of Gibbs except for the first sequence. However, this is justified by that evaluating the conditional distribution of the change point locations is relatively cheap as there are only $49$ possible locations of the change point.\\\\
To further study the actual values of the samples produced, we take a look at the trace plots of each change point location. \cref{appendix:trace_full} lists the trace plots of each change point, grouped by the particular sequences, across all five runs using both methods. After visually assessing these plots, the samples generated before the Makov chains have converges are removed, and the trace plots after removing the burin-in periods are shown in \cref{appendix:trace_burnin}. It is worth noting that this visual assessment is possible because we have knowledge of the ground truth for each sequence. For example, looking at \cref{fig:truth_example} without knowing the true change point locations may lead to the conclusion that the corresponding Markov chain has converged since the beginning of the simulation.\\\\
From these trace plots, we see that the Markov chains in both methods converge almost immediately under the first two sequences. However, for the latter three sequences, MWG more often than not requires more time than Gibbs before reaching the high density regions. This is particularly evident for the sequence with $60$ observations and $3$ change points because the shorter segments make the segment means less stable. Even after the corresponding chains have converged, it is clear under the models with a larger search space of change point locations, MWG moves between states much less frenquently than Gibbs does.\\\\
To better understand the behaviour of the two approaches, we take a closer look at the last round of simulation on the sequence with $60$ observations and $3$ change points (\cref{fig:eg_before,fig:eg_after}). Using the same initialization, initially both MWG and Gibbs completely miss the last change point in the sequence. Indeed, putting a change point at location $30$ corresponds to a local mode in the posterior distribution of the change point locations, regardless of whether it is the corresponding change point that is given this value. As a result, with a uniform proposal on the change point locations, MWG takes a very long time to generate a sample of higher mass from the more than $30000$ possible configurations. Even after the Markov chain of MWG has converged to the region around the global mode, a similar problem is present. Since any set of change points around the global mode has a relatively high mass already, again with the uniform proposal, MWG fails at efficiently exploring this space as the probability of proposing another set of change point locations of comparable mass is low. This is clearly reflected in \cref{fig:eg_after}.\\\\
In comparison, the Gibbs sampler gets out of the local mode much faster than MWG does. And even after the Gibbs sampler has reached the region of the global mode of the posterior, it seems to explore this high mass region a lot more thoroughly. This is because the Gibbs sampler utilizes the full conditional distribution of the change point locations conditioning on all other parameters (segment means). In this particular case, despite missing the last change point completely, it very quickly identifies the first and third change point using the more information conditional distribution of the change point locations. Once two of the three change points have been identified, the segment means are immediately updated to be much closer to the true segment means, which then informs the Gibbs sampler to ultimately get out of the local mode.\\\\
From above, it seems like the more informative conditional distribution of the change point locations puts the Gibbs sampler at an advantage. However, we remind ourselves that the trace plots are compared iteration wise. In other words, even though MWG requires more samples to reach the region around the global mode, samples from MWG are generated much faster than those from Gibbs. We now check whether this faster sampling rate from MWG makes up for the difference in performance mentioned above.\\\\
As mentioned in \cref{sec:prob}, the true posterior distributions are available for each of the five sequences. We then use the plots in \cref{appendix:wass} to check how fast the estimated posteriors from both methods approach the true distribution. This distance is measured using the Wasserstein distance. We see that, except for the sequence with $20000$ observations and $1$ change point, the approximate posteriors computed using the Gibbs sampler is uniformly closer to those of MWG. This is because the Gibbs sampler explores the high mass region of the posterior more thoroughly with the help of the conditional distribution of the change point locations. In the case of \cref{fig:wass_exception}, the absence of the difference as shown in the other figures maybe be due to the fact that the sequence is much longer. Then the distances between the approximate and the true posteriors are averaged off by the vast majority of the change point locations with negligible mass.\\\\
To confirm our speculation above, we check the approximate and true marginal posterior distributions of each change point (\cref{appendix:marginal_post}). A general trend in these plots is that the approximate marginal posterior distributions fom the Gibbs sampler better capture the mode of each marginal distribution than those from MWG. Again this is because the Gibbs sampler explores the region of high mass better than MWG.\\\\
However, it is worth pointing out it is not the case that MWG does not have any advantages over the Gibbs sampler. To see this, we plot the effective sample sizes (ESS) and Monte Carlo standard errors (MCSE) of each sequence studied (shown in \cref{appendix:ess_se}). As suggested by \cite{flegal2010batch}, the ESS and MCSE values are computed using the overlapping batch mean method after splitting the number of observations $n = a_nb_n = \left(n^{\frac{1}{3}}\right)\left(n^{\frac{2}{3}}\right)$. We see that in the sequences with $20000$ observations and $1$ change point and with $100$ observations and $3$ change points, samples from MWG tends to have higher ESS's and lower MCSE's. Both of these sequences have a large search space of change point locations and do not exploit MWG's slow mixing rates. Under such circumstances, if the ground truth is unknown, samples from MWG may shed some more insights than those from Gibbs.
% ...